---
title: 02 - AI Driven Development
---

## The main attention thread

This is where you put your main focus on, that you want to manually orchestrate or needs more granular agent intervention. While you focus on this, you could have long background agents taking care of more trivial stuff. I'm even waiting for [codex](https://openai.com/codex/) to finish an implementation of a new component for the registry as I write this, this is where the multiplier factor of agents kicks in.

## About Writing Code

### Shape the Feature

Even if you are sure on how to implement, use the model to approach the problem, tell it how you'd do it, ask for alternatives, question it's position and _shape the solution_ in collaboration. You'll can get surprised on how much you could be ignoring about the requirement and even if you already thought about everything, you'd be creating valuable context.

### Ask the code

If you are asked to write a feature into an unexplored codebase by yourself, use cursor in "Ask Mode" to make your questions, don't waste time exploring manually, this usually takes you into long sessions of context extensive research that us humans struggle with. AI does a fantastic job on pointing you the right places. You can even take your own notes to keep track of the important stuff.

## Long running agents

This is a new field I dived into recently. I mainly use cursor for my daily workflow, most of my cursor sessions end up in &lt;1min, and I get angry when that doesn't happen. That's because I use cursor as my "main attention thread" tool. I don't want to stare at the chain of thought and tool callings for more than 1min, and cursor does a great job on keeping my intervention span low.

On the opposite side, when cursor takes 3 / 4 mins to do something, and it's taking time from my main thread, I get really mad if it outputs slop. So for long running stuff like refactors or big new features, I prefer to defer that work to a non blocking place. I tested two tools I'd like to introduce you to.

### Codex (GPT 5.1 - Codex)

Codex CLI is pretty reliable when it comes to long running tasks, and this is it's default behavior, most of my codex sessions are about 3/4 minutes, and the results are surprisingly good. Before starting, [codex](https://openai.com/codex/) does an extensive research over the code to find how you do stuff, what are the files involved on the new task and grabs as most context as it can before proceeding. I use this along side my shorter cursor sessions for background local stuff.

![Codex CLI screenshot](/static/c/codex.webp)

### Cursor Cloud Agents

When I want simpler trivial stuff like "Integrate Vercel Analytics" or "Add this new documentation entry" or a simple backlog ticket, I DEFENITELY want to defer this to somewhere else while I take care of more important stuff. "Cursor Cloud Agents" fit the needs pretty well, it doesn't run on your computer, you tell it what to do, it creates a new branch out from your current HEAD branch, and runs in a sandbox in the cloud. When it finishes, it appears as a new PR where you can review the changes and even ask further.

<ImageCols>
  <Image
    src="/static/c/cursor-1.webp"
    alt="Cursor chat"
    width={966}
    height={446}
    unoptimized
  />
  <Image
    src="/static/c/cursor-2.webp"
    alt="Cursor Cloud Agents chat"
    width={966}
    height={446}
    unoptimized
  />
</ImageCols>

Cursor has a [webapp for cloud agents](https://cursor.com/agents), you can use it kickoff new agents and supervise them. This is pretty useful to kickoff an idea of a repo you are not working on right now.

![Cursor Cloud Agents webapp](/static/c/cloud-agents-app.webp)

## Global guidelines (AGENTS.md)

We maintain a comprehensive set of global guidelines that shape how our agents behave. These guidelines are stored in a markdown file that gets loaded into the agent's context. You can explore the complete [AGENTS.md](/toolbox/agents) for more details.

## Referencing documentation

This is how I reference documentation when I’m talking to agents: **if a site offers a `.md` version of a page**, I link that instead of whatever URL I’m currently viewing in the browser.

When it’s available, my standard is: **same path, add `.md`**.

It works better for agent conversations because:

- **It’s the actual content**: clean markdown, not a rendered page with layout/navigation.
- **It’s consistent**: every docs link follows the same shape, so it’s easy to produce and scan.
- **It’s unambiguous context**: dropping a `.md` link in a prompt clearly signals “use this as documentation source”.

This page, for example, [supports this pattern](https://registry.joyco.studio/logs/02-ai-driven-development.md). NextJS docs does support this pattern too.

## About reviews

As a studio we agree on a set of [PR Guidelines](/toolbox/pr-guidelines) to maintain code quality. These are not stuff that a linter could catch, these involve stuff like logic and data management. In the past, I used to review these myself, occupying my "**main attention thread**", but now we have better tools like [Greptile](https://www.greptile.com/), we load a set of guidelines that the model can load to find specific violations on the PR diff. This applies to both human code and agent code, like new PR from the "Cursor Cloud Agent". We collaborate as a team on augmenting these guidelines to refine what we ship while delegating most of these to specialized agents.

![Greptile review](/static/c/greptile-review.webp)

## References

- [Shipping at Inference Speed](https://steipete.me/posts/2025/shipping-at-inference-speed) by Peter Steinberger
